# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Example of a Multi-node worker, but no WideEP or EPLB.
# See wide_ep*.yaml for WideEP example configs.
backend: pytorch
tensor_parallel_size: 16
moe_expert_parallel_size: 16
enable_attention_dp: true
max_batch_size: 256
max_num_tokens: 256
max_seq_len: 8448

kv_cache_config:
  free_gpu_memory_fraction: 0.7
  dtype: fp8

cuda_graph_config:
  enable_padding: true
  batch_sizes:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  - 64
  - 128
  - 256
