1. The number of GPU cards is not equal to tp * dp * ep. Don’t try to set the dp-size and ep-size (these two parameters still exist due to historical reasons), 
suggests that when enabling dp, use --enable-dp-attention,
when enabling ep, use --enable-ep-moe, so that you can intuitively recognize that they belong to the same communication group (that is, the total number of GPU cards is equal to the number of tp, where tp = dp = ep). 

2. EP for DeepSeek r1 is not supported in current version(0.4.3). Refer to https://github.com/sgl-project/sglang/issues/2740. 

3. Not support tp>16 due to bug：https://github.com/sgl-project/sglang/issues/3345 

4. Enable --enable-dp-attention will increase concurrency and TFFT but will gain benefit for throughput when traffic request is big. However, it is not a linearly growing relationship. 
When the traffic is too high, the throughput may decrease. 

5. If you would like to set fixed concurrency, you should set --max-concurrency, setting a reasonable --max-concurrency value can get better throughput and prevent overloading. 

6. When the traffic request rate is relatively high, by fixing all the input parameters, it is found that the larger the osl is, the less the TTFT changes, the worse the TPOT becomes, and the input throughput also deteriorates. 

7. Enable --enable-torch-compile can usually gain performance benefit, however it may not always guarantee a performance boost in all scenarios, in some case, the performance benefit might be negligible.  

8. "--mem-fraction-static 0.8" is for the size proportion of the KV cache pool, which is used to optimize memory usage. For example, --mem-fraction-static 0.8 means that 80% of the memory is allocated to the KV cache pool. If not meet OOM, set mem_fraction_static to a bigger value will gain throughput benefit. 

9. Support custom allreduce in case of tp<8, --disable-custom-all-reduce won’t always harm the performance, because it is a trade-off between resource for compute and for communication. 

10. When you find that the cache hit rate is relatively low, you can consider using the parameter --disable-radix-cache to reduce the overhead, this will gain performance benefit, as the removal of the radix cache in such a situation can free up resources and potentially streamline the caching process. However, you shall not turn on “--disable-radix-cache” if the cache hit rate is high, because when the cache hit rate is already high, the radix cache is functioning effectively and turning it off might disrupt the existing optimized caching mechanism, leading to increased latency and a decrease in overall performance.   

11. Enable quantization: 

--torchao-config: Specify the torch quantization configuration file, which is used for accelerating model quantization. 
--quantization: Set the type of weight quantization. For example, --quantization fp8 means using fp8 weight quantization. 
--kv-cache-dtype: Set the quantization data type of the KV cache. For example, --kv-cache-dtype fp8_e5m2 means using the fp8_e5m2 type of KV cache quantization. 

12. DeepSeek R1 serve crash occasionally 

[Bug] DeepSeek R1 serve crash occasionally on 2*H100 · Issue #3424 · sgl-project/sglang 

Though it is claimed that https://github.com/nvcastet/sglang/tree/fix_all_gather_cuda_graph should fix this problem, but this branch has performance regressom as compared to sglang 0.4.3 

13. MTP is supported in sglang 0.4.3.post2，gain perf increment with small concurrency(<=64) only 

14. Currently, flashinfer mla is not always good, it is especially bad when OSL is big 

[Question] Why is the performance worse after --enable-flashinfer-mla on H20 · Issue #3917 · sgl-project/sglang 

15. After launch kernel, the first test case usually takes longer TTFT. 

16. If stuck in startup, shall try to decrease the chunk_prefill_size and max_running_request 

17. If meet “NCCL WARN Error: failed to extend /dev/shm/nccl-iYZbWq to *** bytes” problem, try to pass “--shm-size=32G” when enter docker image and Open the maximum limit number of ulimit. 

 

 
